# CV Recommendation System ‚Äì Architecture & Fonctionnalit√©s

Ce projet offre une cha√Æne compl√®te pour l‚Äôingestion de CV, leur structuration par LLM et une recommandation hybride combinant recherche vectorielle et re-ranking BERT. L‚Äôobjectif est de proposer rapidement une short-list de candidats pertinents √† partir d‚Äôune requ√™te textuelle (job description, profil cible, etc.).

## Architecture globale
- **Backend FastAPI** : expose les endpoints REST, sert l‚Äôinterface web statique et orchestre le pipeline.
- **Services d‚Äôingestion** : extraction multi-format (PDF, DOCX, images), prompts LLM sp√©cialis√©s, validation Pydantic.
- **Stockage** : MongoDB pour les documents structur√©s et les r√©sum√©s orient√©s matching.
- **Recherche vectorielle** : SentenceTransformer (SBERT) + index FAISS persistant (`cv_index.faiss` + `id_map.pkl`). Prise en charge des embeddings locaux ou h√©berg√©s via Together.ai.
- **Re-ranking** : cross-encoder BERT (`ms-marco-MiniLM-L-6-v2`) avec cache LRU pour acc√©l√©rer les requ√™tes r√©currentes.
- **Interface web** : formulaire JSON align√© sur l‚ÄôAPI, visualisation d√©taill√©e des scores et diagnostics.

## Principales fonctionnalit√©s
- **Ingestion batch** (`POST /api/cv/upload-cv-batch`) : import d‚Äôun ZIP de CV, structuration, enregistrement Mongo et indexation FAISS (OCR multi-langue + contr√¥les qualit√©).
- **Recommandation** (`POST /api/cv/recommend-candidates`) : pipeline en cinq √©tapes (pr√©-traitement requ√™te ‚Üí SBERT/FAISS ‚Üí re-ranking cross-encoder ‚Üí fusion des scores ‚Üí Top‚ÄØ10).
- **Analyse LLM orient√©e exp√©rience** : r√©sum√© analytique, estimation automatique du niveau (junior/mid/senior), extraction hard/soft skills.
- **Interface front** : saisie JSON (poids mots-cl√©s, importance exp√©rience), affichage des composantes de score et export JSON brut.
- **Administration & data cleaning** : stats, purge compl√®te, extraction de num√©ros de t√©l√©phone.

## Flux de traitement ‚Äì Vue rapide
1. **Upload ZIP** ‚Üí extraction texte + prompts LLM ‚Üí stockage Mongo + embeddings FAISS.
2. **Requ√™te utilisateur** ‚Üí normalisation & analyse ‚Üí FAISS Top‚ÄØK (configurable) ‚Üí re-ranking cross-encoder.
3. **Fusion des scores** (embedding, cross-encoder, mots-cl√©s, exp√©rience) ‚Üí Top‚ÄØ10 retourn√© avec diagnostics complets.

---

## Configuration essentielle

| Variable | R√¥le | Valeur par d√©faut |
|----------|------|-------------------|
| `MONGO_URI`, `DB_NAME` | Connexion MongoDB | `mongodb://localhost:27017/`, `cv_recommendation_db` |
| `TOGETHER_API_KEY`, `TOGETHER_MODEL` | Acc√®s LLM pour la structuration | `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` |
| `EMBEDDING_MODEL` | Mod√®le SentenceTransformer ou Together.ai | `sentence-transformers/all-mpnet-base-v2` |
| `RERANKER_MODEL` | Cross-encoder de re-ranking | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| `FAISS_PRESELECTION_K` | Taille pr√©-s√©lection FAISS | `100` |
| `FUSION_*` | Poids des composantes (embedding / cross-encoder / keywords / exp√©rience) | `0.45 / 0.45 / 0.05 / 0.05` |
| `EMBEDDING_PROVIDER` | `local` ou `together` | `local` |
| `OCR_LANGUAGES` | Langues OCR Tesseract (codes concat√©n√©s) | `eng+fra` |
| `OCR_MIN_CONFIDENCE` | Seuil de confiance OCR (0-100) | `35.0` |
| `CALIBRATION_PROFILE_PATH` | Profil de calibration JSON | `calibration/weights.json` |

Les poids sont normalis√©s automatiquement et peuvent √™tre ajust√©s √† l‚Äôaide d‚Äôun jeu d‚Äôor (pr√©cision@K, nDCG). L‚Äô√©v√©nement `startup_event` chauffe les mod√®les pour limiter la latence du premier appel, et un cache LRU √©vite de recalculer les scores cross-encoder r√©p√©t√©s.

### Calibration guid√©e
Un script d√©di√© permet de calibrer automatiquement les poids sur un corpus annot√© :

```bash
python scripts/calibrate_weights.py --dataset data/annotated_samples.jsonl --grid-step 0.05
```

Le profil g√©n√©r√© est enregistr√© dans `calibration/weights.json` puis charg√© automatiquement au d√©marrage.

---

## D√©tails du pipeline d‚Äôingestion

### Pipeline complet

Ce projet permet d‚Äôanalyser automatiquement un lot de CV (en PDF, DOCX ou image) afin d‚Äôen extraire des informations structur√©es, de les enrichir via un LLM (mod√®le d‚Äôintelligence artificielle), et de les indexer pour une recherche vectorielle efficace.

---

##  Fonctionnalit√© principale

L‚ÄôAPI expose un endpoint unique :

### `POST /api/cv/upload-cv-batch`

Ce point d‚Äôentr√©e accepte un **fichier ZIP** contenant plusieurs CV et effectue automatiquement l‚Äôensemble du pipeline suivant :

---

## üîç D√©roul√© complet c√¥t√© serveur

### 1Ô∏è Lecture du ZIP
- Le fichier ZIP est lu **en m√©moire** (`zip_bytes`).
- Ouverture avec `zipfile.ZipFile` et it√©ration sur chaque fichier contenu.
- Les **dossiers internes** sont ignor√©s.
- Chaque **fichier individuel (PDF, DOCX, image)** est trait√© s√©par√©ment.

---

### 2Ô∏è Extraction du texte brut (`extract_text_from_cv`)
- **PDF** ‚Üí texte via **PyMuPDF** (avec fallback OCR multi-langue via **pytesseract** configurable).  
- **DOCX** ‚Üí extraction via **python-docx**.  
- **Images (JPG, PNG, etc.)** ‚Üí OCR avec contr√¥le de confiance (seuil `OCR_MIN_CONFIDENCE`).  
- R√©-ex√©cution OCR avec param√®tres optimis√©s si la confiance est basse.  
- Nettoyage du texte (normalisation unicode, ponctuation). CV vides ou non reconnaissables ‚Üí ignor√©s.

---

### 3Ô∏è Structuration du contenu (`extract_structured_info`)
- Le texte du CV est envoy√© au **LLM** (mod√®le de langage) via un **prompt sp√©cialis√©**.
- Le LLM retourne un **JSON structur√©** contenant :
  - Identit√© (nom, email, t√©l√©phone)
  - √âducation
  - Exp√©riences professionnelles
  - Comp√©tences techniques (hard skills)
  - Comp√©tences comportementales (soft skills)
  - Langues, projets, etc.
- Le JSON est **nettoy√© et valid√©** par le mod√®le **Pydantic `CVInfo`**.
- Post-traitements automatiques : d√©duplication des exp√©riences, v√©rification des dates et ajout √©ventuel de `validationWarnings`.
- En cas de JSON invalide ou sch√©ma non conforme ‚Üí CV **ignor√©** et erreur **logg√©e**.

---

### 4Ô∏è Enregistrement dans MongoDB
- Insertion du document structur√© dans la collection principale (`collection.insert_one`).
- L‚ÄôID Mongo (`inserted_id`) est sauvegard√© pour le relier aux √©tapes suivantes.

---

### 5Ô∏è Analyse orient√©e exp√©rience (`extract_experience_summary`)
- Un **second prompt LLM** est utilis√© pour extraire :
  - Le **total d‚Äôann√©es d‚Äôexp√©rience**
  - Le **niveau de s√©niorit√©** (`junior`, `mid`, `senior`)
  - Un **r√©sum√© professionnel clair**
  - Les **comp√©tences techniques et comportementales**
- Validation via **Pydantic `CVSummary`** + contr√¥les (valeur d‚Äôann√©es non n√©gative, duplication de phrases).
- En cas d‚Äô√©chec ‚Üí CV **ignor√©** et erreur **signal√©e**.

---

### 6Ô∏è Pr√©paration du texte pour embedding (`build_text_from_json`)
- Construction d‚Äôun **texte pond√©r√©** combinant :
  - Exp√©riences d√©taill√©es (r√©p√©t√©es 3√ó pour donner plus de poids)
  - Ann√©es d‚Äôexp√©rience
  - Niveau de s√©niorit√©
  - Hard/Soft skills
  - √âducation et r√©sum√©
- Le tout fusionn√© dans une seule cha√Æne de texte pr√™te pour l‚Äôencodage vectoriel.

---

### 7Ô∏è Encodage vectoriel
- Le texte final est encod√© avec un mod√®le **SentenceTransformer** avanc√© (par d√©faut `all-mpnet-base-v2`) ou un mod√®le h√©berg√© via Together.ai.
- Les vecteurs sont normalis√©s puis :
  - Convertis en **liste JSON** (pour stockage Mongo)
  - Ins√©r√©s dans la collection `collection_embedded_data`.

---

### 8Ô∏è Indexation FAISS
- Chargement (ou cr√©ation si inexistant) de l‚Äôindex :
  - `cv_index.faiss` (vecteurs)
  - `id_map.pkl` (correspondance vecteur ‚Üî ID Mongo)
- Normalisation L2 du vecteur puis ajout √† l‚Äôindex.
- Mise √† jour de la **map d‚ÄôIDs** et sauvegarde sur disque.

---

### 9 Reporting final
- Comptage du nombre total :
  - CV trait√©s avec succ√®s
  - CV ignor√©s (erreurs, doublons, fichiers vides)
- Enregistrement des r√©sultats dans une liste `results` :
  - `filename`
  - `status` (`success`, `error`, `skipped`)
  - Messages ou raisons d‚Äô√©chec
- R√©ponse **JSON** envoy√©e au client (incluant les `validationWarnings` √©ventuels pour suivi de qualit√©) :

```json
{
  "status": "completed",
  "total_processed": 42,
  "total_skipped": 8,
  "message": "Successfully processed 42 CVs, skipped 8",
  "results": [...]
}
