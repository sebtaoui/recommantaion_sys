# CV Processing Pipeline

Ce projet permet d‚Äôanalyser automatiquement un lot de CV (en PDF, DOCX ou image) afin d‚Äôen extraire des informations structur√©es, de les enrichir via un LLM (mod√®le d‚Äôintelligence artificielle), et de les indexer pour une recherche vectorielle efficace.

---

##  Fonctionnalit√© principale

L‚ÄôAPI expose un endpoint unique :

### `POST /api/cv/upload-cv-batch`

Ce point d‚Äôentr√©e accepte un **fichier ZIP** contenant plusieurs CV et effectue automatiquement l‚Äôensemble du pipeline suivant :

---

## üîç D√©roul√© complet c√¥t√© serveur

### 1Ô∏è Lecture du ZIP
- Le fichier ZIP est lu **en m√©moire** (`zip_bytes`).
- Ouverture avec `zipfile.ZipFile` et it√©ration sur chaque fichier contenu.
- Les **dossiers internes** sont ignor√©s.
- Chaque **fichier individuel (PDF, DOCX, image)** est trait√© s√©par√©ment.

---

### 2Ô∏è Extraction du texte brut (`extract_text_from_cv`)
- **PDF** ‚Üí texte via **PyMuPDF** (avec fallback OCR via **pytesseract** si besoin).  
- **DOCX** ‚Üí extraction via **python-docx**.  
- **Images (JPG, PNG, etc.)** ‚Üí OCR avec **pytesseract**.  
- Nettoyage l√©ger du texte : suppression des espaces multiples, normalisation, etc.
- Si le texte extrait est vide ‚Üí CV **ignor√©** (`"Skipping empty CV"`).

---

### 3Ô∏è Structuration du contenu (`extract_structured_info`)
- Le texte du CV est envoy√© au **LLM** (mod√®le de langage) via un **prompt sp√©cialis√©**.
- Le LLM retourne un **JSON structur√©** contenant :
  - Identit√© (nom, email, t√©l√©phone)
  - √âducation
  - Exp√©riences professionnelles
  - Comp√©tences techniques (hard skills)
  - Comp√©tences comportementales (soft skills)
  - Langues, projets, etc.
- Le JSON est **nettoy√© et valid√©** par le mod√®le **Pydantic `CVInfo`**.
- En cas de JSON invalide ou sch√©ma non conforme ‚Üí CV **ignor√©** et erreur **logg√©e**.

---

### 4Ô∏è Enregistrement dans MongoDB
- Insertion du document structur√© dans la collection principale (`collection.insert_one`).
- L‚ÄôID Mongo (`inserted_id`) est sauvegard√© pour le relier aux √©tapes suivantes.

---

### 5Ô∏è Analyse orient√©e exp√©rience (`extract_experience_summary`)
- Un **second prompt LLM** est utilis√© pour extraire :
  - Le **total d‚Äôann√©es d‚Äôexp√©rience**
  - Le **niveau de s√©niorit√©** (`junior`, `mid`, `senior`)
  - Un **r√©sum√© professionnel clair**
  - Les **comp√©tences techniques et comportementales**
- Validation via **Pydantic `CVSummary`**.
- En cas d‚Äô√©chec ‚Üí CV **ignor√©** et erreur **signal√©e**.

---

### 6Ô∏è Pr√©paration du texte pour embedding (`build_text_from_json`)
- Construction d‚Äôun **texte pond√©r√©** combinant :
  - Exp√©riences d√©taill√©es (r√©p√©t√©es 3√ó pour donner plus de poids)
  - Ann√©es d‚Äôexp√©rience
  - Niveau de s√©niorit√©
  - Hard/Soft skills
  - √âducation et r√©sum√©
- Le tout fusionn√© dans une seule cha√Æne de texte pr√™te pour l‚Äôencodage vectoriel.

---

### 7Ô∏è Encodage vectoriel
- Le texte final est encod√© avec le mod√®le **SentenceTransformer**  
  (`all-MiniLM-L6-v2`, `normalize_embeddings=True`).
- Le vecteur obtenu est :
  - Converti en **liste JSON** (pour stockage Mongo)
  - Ins√©r√© dans la collection `collection_embedded_data`.

---

### 8Ô∏è Indexation FAISS
- Chargement (ou cr√©ation si inexistant) de l‚Äôindex :
  - `cv_index.faiss` (vecteurs)
  - `id_map.pkl` (correspondance vecteur ‚Üî ID Mongo)
- Normalisation L2 du vecteur puis ajout √† l‚Äôindex.
- Mise √† jour de la **map d‚ÄôIDs** et sauvegarde sur disque.

---

### 9 Reporting final
- Comptage du nombre total :
  - CV trait√©s avec succ√®s
  - CV ignor√©s (erreurs, doublons, fichiers vides)
- Enregistrement des r√©sultats dans une liste `results` :
  - `filename`
  - `status` (`success`, `error`, `skipped`)
  - Messages ou raisons d‚Äô√©chec
- R√©ponse **JSON** envoy√©e au client :

```json
{
  "status": "completed",
  "total_processed": 42,
  "total_skipped": 8,
  "message": "Successfully processed 42 CVs, skipped 8",
  "results": [...]
}
